#!/usr/bin/env python3
"""
–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
–í–µ–¥–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–±–æ—Ç—ã –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏ —Å–æ–∑–¥–∞–µ—Ç –æ—Ç—á–µ—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
"""

import json
import sqlite3
import os
import re
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from urllib.parse import urlparse

class LongTermMonitor:
    def __init__(self, instance_path='.'):
        self.instance_path = instance_path
        self.db_file = os.path.join(instance_path, 'source_monitoring.db')
        self.log_file = os.path.join(instance_path, 'monitor.log')
        self.feeds_file = os.path.join(instance_path, 'feeds.txt')
        self.report_file = os.path.join(instance_path, 'source_health_report.json')
        
        self.init_database()
        
    def init_database(self):
        """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–∏ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS source_attempts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_url TEXT NOT NULL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                status TEXT NOT NULL,  -- 'success', 'error', '404', 'timeout', etc.
                articles_found INTEGER DEFAULT 0,
                error_message TEXT,
                response_time REAL,
                attempt_number INTEGER DEFAULT 1
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS daily_stats (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_url TEXT NOT NULL,
                date DATE NOT NULL,
                total_attempts INTEGER DEFAULT 0,
                successful_attempts INTEGER DEFAULT 0,
                error_attempts INTEGER DEFAULT 0,
                total_articles INTEGER DEFAULT 0,
                avg_response_time REAL,
                main_error_type TEXT,
                reliability_score REAL,  -- –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
                UNIQUE(source_url, date)
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ –∏—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS source_config (
                source_url TEXT PRIMARY KEY,
                domain TEXT NOT NULL,
                is_active BOOLEAN DEFAULT 1,
                first_seen DATE DEFAULT CURRENT_DATE,
                last_seen DATE DEFAULT CURRENT_DATE,
                notes TEXT  -- –¥–ª—è –∑–∞–ø–∏—Å–∏ —Ä—É—á–Ω—ã—Ö –∑–∞–º–µ—Ç–æ–∫
            )
        ''')
        
        conn.commit()
        conn.close()
        
    def parse_current_logs(self, hours_back=24):
        """–ü–∞—Ä—Å–∏—Ç –ª–æ–≥–∏ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤ –ë–î"""
        print(f"üìä –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {hours_back} —á–∞—Å–æ–≤...")
        
        if not os.path.exists(self.log_file):
            print("‚ùå –§–∞–π–ª –ª–æ–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
            
        # –ß–∏—Ç–∞–µ–º –ª–æ–≥–∏
        with open(self.log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        parsed_count = 0
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        
        for line in lines[-10000:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10–∫ —Å—Ç—Ä–æ–∫
            # –ü–∞—Ä—Å–∏–º —É—Å–ø–µ—à–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            success_match = re.search(r'‚úÖ (https?://[^\s:]+).*?(\d+) –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π', line)
            if success_match:
                url, articles = success_match.groups()
                
                cursor.execute('''
                    INSERT INTO source_attempts 
                    (source_url, status, articles_found, timestamp)
                    VALUES (?, ?, ?, ?)
                ''', (url, 'success', int(articles), datetime.now()))
                parsed_count += 1
                
            # –ü–∞—Ä—Å–∏–º –æ—à–∏–±–∫–∏ 404
            error_404_match = re.search(r'‚ùå.*RSS.*404.*?(https?://[^\s]+)', line)
            if error_404_match:
                url = error_404_match.group(1)
                
                cursor.execute('''
                    INSERT INTO source_attempts 
                    (source_url, status, error_message, timestamp)
                    VALUES (?, ?, ?, ?)
                ''', (url, '404', 'RSS –Ω–µ –Ω–∞–π–¥–µ–Ω (404)', datetime.now()))
                parsed_count += 1
                
            # –ü–∞—Ä—Å–∏–º —Ç–∞–π–º–∞—É—Ç—ã
            timeout_match = re.search(r'‚è∞ –¢–∞–π–º–∞—É—Ç –¥–ª—è (https?://[^\s]+)', line)
            if timeout_match:
                url = timeout_match.group(1)
                
                cursor.execute('''
                    INSERT INTO source_attempts 
                    (source_url, status, error_message, timestamp)
                    VALUES (?, ?, ?, ?)
                ''', (url, 'timeout', '–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –≤—Ä–µ–º–µ–Ω–∏', datetime.now()))
                parsed_count += 1
                
            # –ü–∞—Ä—Å–∏–º —Å–µ—Ç–µ–≤—ã–µ –æ—à–∏–±–∫–∏
            network_match = re.search(r'üåê –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ (https?://[^\s]+)', line)
            if network_match:
                url = network_match.group(1)
                
                cursor.execute('''
                    INSERT INTO source_attempts 
                    (source_url, status, error_message, timestamp)
                    VALUES (?, ?, ?, ?)
                ''', (url, 'network_error', '–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞', datetime.now()))
                parsed_count += 1
                
        conn.commit()
        conn.close()
        
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {parsed_count} –∑–∞–ø–∏—Å–µ–π")
        
    def update_daily_stats(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        print("üìà –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
        
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞ —Å–µ–≥–æ–¥–Ω—è
        today = datetime.now().date()
        
        cursor.execute('''
            SELECT 
                source_url,
                COUNT(*) as total_attempts,
                SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as successful,
                SUM(CASE WHEN status != 'success' THEN 1 ELSE 0 END) as errors,
                SUM(articles_found) as total_articles,
                AVG(response_time) as avg_time
            FROM source_attempts 
            WHERE DATE(timestamp) = ?
            GROUP BY source_url
        ''', (today,))
        
        stats = cursor.fetchall()
        
        for stat in stats:
            url, total, successful, errors, articles, avg_time = stat
            
            reliability = (successful / total * 100) if total > 0 else 0
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∏–ø –æ—à–∏–±–∫–∏
            cursor.execute('''
                SELECT status, COUNT(*) as count
                FROM source_attempts 
                WHERE source_url = ? AND DATE(timestamp) = ? AND status != 'success'
                GROUP BY status
                ORDER BY count DESC
                LIMIT 1
            ''', (url, today))
            
            main_error_result = cursor.fetchone()
            main_error = main_error_result[0] if main_error_result else None
            
            # –í—Å—Ç–∞–≤–ª—è–µ–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º –µ–∂–µ–¥–Ω–µ–≤–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            cursor.execute('''
                INSERT OR REPLACE INTO daily_stats 
                (source_url, date, total_attempts, successful_attempts, 
                 error_attempts, total_articles, avg_response_time, 
                 main_error_type, reliability_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (url, today, total, successful, errors, articles or 0, 
                  avg_time, main_error, reliability))
                  
        conn.commit()
        conn.close()
        
    def load_source_config(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ feeds.txt"""
        print("üìã –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
        
        if not os.path.exists(self.feeds_file):
            return
            
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        with open(self.feeds_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line.startswith('http'):
                    # –ê–∫—Ç–∏–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
                    domain = urlparse(line).netloc
                    cursor.execute('''
                        INSERT OR IGNORE INTO source_config (source_url, domain, is_active)
                        VALUES (?, ?, 1)
                    ''', (line, domain))
                    
                elif line.startswith('#') and 'http' in line:
                    # –û—Ç–∫–ª—é—á–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
                    url_match = re.search(r'https?://[^\s]+', line)
                    if url_match:
                        url = url_match.group()
                        domain = urlparse(url).netloc
                        cursor.execute('''
                            INSERT OR IGNORE INTO source_config (source_url, domain, is_active)
                            VALUES (?, ?, 0)
                        ''', (url, domain))
                        
        conn.commit()
        conn.close()
        
    def generate_report(self, days_back=7):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥"""
        print(f"üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –∑–∞ {days_back} –¥–Ω–µ–π...")
        
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=days_back)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∑–∞ –ø–µ—Ä–∏–æ–¥
        cursor.execute('''
            SELECT 
                sc.source_url,
                sc.domain,
                sc.is_active,
                COUNT(ds.date) as days_with_data,
                AVG(ds.reliability_score) as avg_reliability,
                SUM(ds.successful_attempts) as total_success,
                SUM(ds.error_attempts) as total_errors,
                SUM(ds.total_articles) as total_articles,
                GROUP_CONCAT(DISTINCT ds.main_error_type) as error_types
            FROM source_config sc
            LEFT JOIN daily_stats ds ON sc.source_url = ds.source_url 
                AND ds.date BETWEEN ? AND ?
            GROUP BY sc.source_url
            ORDER BY avg_reliability DESC NULLS LAST
        ''', (start_date, end_date))
        
        sources_data = cursor.fetchall()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
        report = {
            'generated_at': datetime.now().isoformat(),
            'period': {
                'start_date': start_date.isoformat(),
                'end_date': end_date.isoformat(),
                'days': days_back
            },
            'summary': {},
            'sources': {}
        }
        
        all_sources = []
        problematic_sources = []
        excellent_sources = []
        
        for row in sources_data:
            url, domain, is_active, days_data, avg_rel, success, errors, articles, error_types = row
            
            reliability = avg_rel if avg_rel is not None else 0
            total_attempts = (success or 0) + (errors or 0)
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            if reliability >= 90:
                status = "üü¢ –û—Ç–ª–∏—á–Ω—ã–π"
                category = "excellent"
                excellent_sources.append(url)
            elif reliability >= 70:
                status = "üü° –•–æ—Ä–æ—à–∏–π"
                category = "good"
            elif reliability >= 40:
                status = "üü† –ü—Ä–æ–±–ª–µ–º–Ω—ã–π"
                category = "problematic"
                problematic_sources.append(url)
            else:
                status = "üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π"
                category = "critical"
                problematic_sources.append(url)
                
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            recommendations = []
            if not is_active:
                recommendations.append("‚è∏Ô∏è –ò—Å—Ç–æ—á–Ω–∏–∫ –æ—Ç–∫–ª—é—á–µ–Ω - –≤–æ–∑–º–æ–∂–Ω–æ —Å—Ç–æ–∏—Ç –ø–µ—Ä–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å")
            elif reliability < 50:
                if '404' in (error_types or ''):
                    recommendations.append("üîß –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å URL - –≤–æ–∑–º–æ–∂–Ω–æ RSS –ø–µ—Ä–µ–µ—Ö–∞–ª")
                if 'timeout' in (error_types or ''):
                    recommendations.append("‚ö° –£–≤–µ–ª–∏—á–∏—Ç—å timeout –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å retry –ª–æ–≥–∏–∫—É")
                if 'network_error' in (error_types or ''):
                    recommendations.append("üåê –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç—å—é - –≤–æ–∑–º–æ–∂–Ω–æ –Ω—É–∂–µ–Ω –ø—Ä–æ–∫—Å–∏")
                    
            if not recommendations and reliability < 90:
                recommendations.append("üîç –¢—Ä–µ–±—É–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")
            elif not recommendations:
                recommendations.append("‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ")
                
            source_info = {
                'domain': domain,
                'is_active': bool(is_active),
                'status': status,
                'category': category,
                'reliability_score': round(reliability, 1),
                'days_monitored': days_data or 0,
                'total_attempts': total_attempts,
                'successful_attempts': success or 0,
                'error_attempts': errors or 0,
                'articles_found': articles or 0,
                'main_errors': error_types.split(',') if error_types else [],
                'recommendations': recommendations
            }
            
            report['sources'][url] = source_info
            all_sources.append(url)
            
        # –û–±—â–∞—è —Å–≤–æ–¥–∫–∞
        report['summary'] = {
            'total_sources': len(all_sources),
            'excellent_sources': len(excellent_sources),
            'problematic_sources': len(problematic_sources),
            'avg_reliability': round(sum(s['reliability_score'] for s in report['sources'].values()) / len(all_sources), 1) if all_sources else 0
        }
        
        conn.close()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        with open(self.report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
            
        return report
        
    def print_summary_report(self, days_back=7):
        """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç –≤ –∫–æ–Ω—Å–æ–ª—å"""
        report = self.generate_report(days_back)
        
        print(f"\nüìä –û–¢–ß–ï–¢ –ü–û –ò–°–¢–û–ß–ù–ò–ö–ê–ú –∑–∞ {days_back} –¥–Ω–µ–π")
        print("=" * 60)
        print(f"üìÖ –ü–µ—Ä–∏–æ–¥: {report['period']['start_date']} ‚Üí {report['period']['end_date']}")
        print(f"üì° –í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {report['summary']['total_sources']}")
        print(f"üü¢ –û—Ç–ª–∏—á–Ω—ã—Ö: {report['summary']['excellent_sources']}")
        print(f"üî¥ –ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö: {report['summary']['problematic_sources']}")
        print(f"üìà –°—Ä–µ–¥–Ω—è—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å: {report['summary']['avg_reliability']}%")
        
        # –¢–æ–ø –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        problematic = [(url, data) for url, data in report['sources'].items() 
                      if data['category'] in ['problematic', 'critical']]
        problematic.sort(key=lambda x: x[1]['reliability_score'])
        
        if problematic:
            print(f"\n‚ö†Ô∏è –¢–†–ï–ë–£–Æ–¢ –í–ù–ò–ú–ê–ù–ò–Ø ({len(problematic)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤):")
            print("-" * 60)
            for url, data in problematic[:10]:  # –¢–æ–ø-10 –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö
                domain = data['domain'][:35]
                reliability = data['reliability_score']
                status = data['status']
                articles = data['articles_found']
                
                print(f"{status:<12} {domain:<35} {reliability:>5.1f}% ({articles} —Å—Ç–∞—Ç–µ–π)")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                for rec in data['recommendations'][:2]:  # –ü–µ—Ä–≤—ã–µ 2 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                    print(f"             ‚Üí {rec}")
                print()
                
        print(f"\nüíæ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {self.report_file}")
        
    def run_full_analysis(self, hours_back=24, days_report=7):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
        print("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
        
        self.parse_current_logs(hours_back)
        self.update_daily_stats()
        self.load_source_config()
        self.print_summary_report(days_report)

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤')
    parser.add_argument('--path', default='.', help='–ü—É—Ç—å –∫ —ç–∫–∑–µ–º–ø–ª—è—Ä—É')
    parser.add_argument('--hours', type=int, default=24, help='–ß–∞—Å–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤')
    parser.add_argument('--days', type=int, default=7, help='–î–Ω–µ–π –¥–ª—è –æ—Ç—á–µ—Ç–∞')
    parser.add_argument('--report-only', action='store_true', help='–¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å –æ—Ç—á–µ—Ç')
    
    args = parser.parse_args()
    
    monitor = LongTermMonitor(args.path)
    
    if args.report_only:
        monitor.print_summary_report(args.days)
    else:
        monitor.run_full_analysis(args.hours, args.days)

if __name__ == "__main__":
    main()