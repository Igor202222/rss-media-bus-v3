import feedparser
import aiohttp
import asyncio
import sqlite3
import time
from datetime import datetime, timezone, timedelta
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urlparse
import traceback

class AsyncRSSParser:
    def __init__(self, db_manager, telegram_sender=None, keywords=None, config=None):
        self.db = db_manager
        self.telegram = telegram_sender
        self.keywords = keywords or []
        self.config = config
        
        # –ù–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞: –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        self.source_modes = self._load_source_modes(config)
        
        # –°—Ç–∞—Ä–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.forward_all = self._determine_forward_mode(config)
        self.filter_by_keywords = not self.forward_all
        
        # –°—á–µ—Ç—á–∏–∫–∏ –æ—à–∏–±–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        self.error_counts = {}
        self.last_error_time = {}
        
        mode_str = self._get_mode_description()
        print(f"üîç AsyncRSSParser —Ä–µ–∂–∏–º: {mode_str}")
        
    def _load_source_modes(self, config):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞"""
        if not config or not hasattr(config, 'SOURCE_MODES'):
            return {}
        
        source_modes = getattr(config, 'SOURCE_MODES', {})
        if source_modes:
            print(f"üéØ –ù–∞–π–¥–µ–Ω—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è {len(source_modes)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            for domain, settings in source_modes.items():
                if domain != 'default':
                    mode = settings.get('mode', 'filter')
                    topic = settings.get('topic_id', 'None')
                    print(f"  üì° {domain}: {mode}, —Ç–æ–ø–∏–∫ {topic}")
        
        return source_modes
    
    def _get_mode_description(self):
        """–û–ø–∏—Å–∞–Ω–∏–µ —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã"""
        if self.source_modes:
            return f"–ì–∏–±–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ({len(self.source_modes)} –ø—Ä–∞–≤–∏–ª)"
        elif self.forward_all:
            return "–í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏"
        else:
            return "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"
    
    def _determine_forward_mode(self, config):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        if not config:
            return len(self.keywords) == 0
        
        if hasattr(config, 'FORWARD_ALL_NEWS'):
            return config.FORWARD_ALL_NEWS
        
        if hasattr(config, 'FILTER_BY_KEYWORDS'):
            return not config.FILTER_BY_KEYWORDS
        
        return len(self.keywords) == 0
    
    def _get_source_settings(self, feed_url):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        if not self.source_modes:
            # –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            return {
                'mode': 'forward_all' if self.forward_all else 'filter',
                'keywords': self.keywords,
                'topic_id': getattr(self.config, 'TELEGRAM_TOPIC_ID', None)
            }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–º–µ–Ω –∏–∑ URL
        try:
            domain = urlparse(feed_url).netloc.lower()
            domain = domain.replace('www.', '')
        except:
            domain = feed_url
        
        # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞
        for source_domain, settings in self.source_modes.items():
            if source_domain == 'default':
                continue
            if source_domain in domain:
                # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                source_settings = {
                    'mode': settings.get('mode', 'filter'),
                    'keywords': settings.get('keywords', self.keywords),
                    'topic_id': settings.get('topic_id', None)
                }
                print(f"üéØ {domain}: —Ä–µ–∂–∏–º '{source_settings['mode']}', —Ç–æ–ø–∏–∫ {source_settings['topic_id']}")
                return source_settings
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        default_settings = self.source_modes.get('default', {})
        return {
            'mode': default_settings.get('mode', 'filter'),
            'keywords': default_settings.get('keywords', self.keywords),
            'topic_id': default_settings.get('topic_id', None)
        }
        
    async def parse_all_feeds_async(self, feeds):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        if not feeds:
            print("‚ö†Ô∏è –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            return 0
        
        print(f"üì° –ù–∞—á–∏–Ω–∞—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(feeds)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–æ–≤
        timeout = aiohttp.ClientTimeout(total=30)
        connector = aiohttp.TCPConnector(
            limit=10, 
            limit_per_host=3,
            force_close=True,
            enable_cleanup_closed=True
        )
        
        headers = {
            'User-Agent': 'RSS Media Monitor/2.0 (Enhanced AsyncRSSParser)',
            'Accept': 'application/rss+xml, application/xml, text/xml, */*',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }
        
        async with aiohttp.ClientSession(
            timeout=timeout, 
            connector=connector,
            headers=headers
        ) as session:
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            tasks = []
            for feed_info in feeds:
                feed_id, feed_url = feed_info[0], feed_info[1]
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å —á–∞—Å—Ç—ã–º–∏ –æ—à–∏–±–∫–∞–º–∏
                if self.should_skip_feed(feed_url):
                    continue
                
                task = self._parse_single_feed_async(session, feed_id, feed_url)
                tasks.append(task)
            
            if not tasks:
                print("‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                return 0
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
            semaphore = asyncio.Semaphore(5)
            
            async def limited_task(task):
                async with semaphore:
                    return await task
            
            limited_tasks = [limited_task(task) for task in tasks]
            results = await asyncio.gather(*limited_tasks, return_exceptions=True)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            total_new_articles = 0
            successful_feeds = 0
            failed_feeds = 0
            
            for i, result in enumerate(results):
                if i < len(feeds):
                    feed_url = feeds[i][1]
                    if isinstance(result, Exception):
                        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {feed_url}: {result}")
                        failed_feeds += 1
                        self._record_feed_error(feed_url)
                    else:
                        total_new_articles += result
                        successful_feeds += 1
                        self._reset_feed_errors(feed_url)
            
            print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful_feeds} —É—Å–ø–µ—à–Ω–æ, {failed_feeds} —Å –æ—à–∏–±–∫–∞–º–∏")
            print(f"üì∞ –í—Å–µ–≥–æ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π: {total_new_articles}")
            
            return total_new_articles
    
    async def _parse_single_feed_async(self, session, feed_id, feed_url):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        max_retries = 3
        retry_delay = 2
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        source_settings = self._get_source_settings(feed_url)
        
        for attempt in range(max_retries):
            try:
                print(f"üì° –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}: {feed_url}")
                
                # –°–∫–∞—á–∏–≤–∞–µ–º RSS
                async with session.get(feed_url) as response:
                    if response.status == 404:
                        print(f"‚ùå RSS –Ω–µ –Ω–∞–π–¥–µ–Ω (404): {feed_url}")
                        return 0
                    elif response.status >= 400:
                        raise aiohttp.ClientResponseError(
                            request_info=response.request_info,
                            history=response.history,
                            status=response.status
                        )
                    
                    content = await response.text(encoding='utf-8', errors='ignore')
                    
                    if not content or len(content) < 100:
                        raise Exception("–ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç")
                
                # –ü–∞—Ä—Å–∏–º RSS –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                loop = asyncio.get_event_loop()
                with ThreadPoolExecutor(max_workers=2) as executor:
                    feed_data = await loop.run_in_executor(
                        executor, self._safe_parse_feed, content
                    )
                
                if not feed_data or not feed_data.entries:
                    print(f"‚ö†Ô∏è RSS –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π: {feed_url}")
                    return 0
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ
                feed_title = getattr(feed_data.feed, 'title', self._extract_domain_name(feed_url))
                self.db.update_feed_info(feed_id, title=feed_title)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å–∏ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                new_articles_count = await self._process_articles_async(
                    feed_id, feed_url, feed_data.entries, source_settings
                )
                
                print(f"‚úÖ {feed_url}: {new_articles_count} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π")
                return new_articles_count
                
            except asyncio.TimeoutError:
                print(f"‚è∞ –¢–∞–π–º–∞—É—Ç –¥–ª—è {feed_url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                
            except aiohttp.ClientError as e:
                print(f"üåê –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ {feed_url}: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {feed_url}: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
        
        print(f"üí• –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã –¥–ª—è {feed_url}")
        return 0
    
    def _safe_parse_feed(self, content):
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ RSS —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            return feedparser.parse(content)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ feedparser: {e}")
            return None
    
    async def _process_articles_async(self, feed_id, feed_url, entries, source_settings):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ RSS —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        new_articles_count = 0
        articles_to_send = []
        
        # –ü–æ–ª—É—á–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç —Å—Ç–∞—Ç–µ–π
        max_age_hours = getattr(self.config, 'MAX_ARTICLE_AGE_HOURS', 24)
        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=max_age_hours)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å–∏ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (—Å—Ç–∞—Ä—ã–µ –ø–µ—Ä–≤—ã–º–∏)
        for entry in entries[:50]:
            try:
                article_data = self._extract_article_data(entry)
                if not article_data:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç —Å—Ç–∞—Ç—å–∏
                if article_data.get('published_date'):
                    if article_data['published_date'] < cutoff_time:
                        continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏
                if self.db.article_exists(article_data.get('link')):
                    continue
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —Å—Ç–∞—Ç—å—é —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                should_send, matched_keywords = self._should_send_article(article_data, source_settings)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                article_id = self.db.add_article(
                    feed_id=feed_id,
                    title=article_data['title'],
                    link=article_data['link'],
                    description=article_data['description'],
                    content=article_data['content'],
                    author=article_data['author'],
                    published_date=article_data['published_date']
                )
                
                if article_id:
                    new_articles_count += 1
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ—Ç–ø—Ä–∞–≤–∫–µ –µ—Å–ª–∏ –ø–æ–¥—Ö–æ–¥–∏—Ç
                    if should_send:
                        article_data['matched_keywords'] = matched_keywords
                        article_data['source_url'] = feed_url
                        article_data['topic_id'] = source_settings.get('topic_id')
                        articles_to_send.append(article_data)
                        
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
                continue
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—å–∏ –≤ Telegram –µ—Å–ª–∏ –µ—Å—Ç—å
        if articles_to_send and self.telegram:
            await self._send_articles_to_telegram(articles_to_send, feed_url)
        
        return new_articles_count
    
    def _should_send_article(self, article_data, source_settings):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω—É–∂–Ω–æ –ª–∏ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —Å—Ç–∞—Ç—å—é —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        mode = source_settings.get('mode', 'filter')
        keywords = source_settings.get('keywords', [])
        
        if mode == 'forward_all':
            # –†–µ–∂–∏–º "–≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏" –¥–ª—è —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            return True, ["–í–°–ï –ù–û–í–û–°–¢–ò"]
        
        elif mode == 'filter':
            # –†–µ–∂–∏–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            if not keywords:
                return True, []
            
            matched_keywords = self._check_keywords(article_data, keywords)
            return len(matched_keywords) > 0, matched_keywords
        
        # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∂–∏–º - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
        return False, []
    
    def _extract_article_data(self, entry):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ RSS —Å—Ç–∞—Ç—å–∏"""
        try:
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title = entry.get('title', '').strip()
            if not title:
                return None
            
            # –°—Å—ã–ª–∫–∞
            link = entry.get('link', '').strip()
            
            # –û–ø–∏—Å–∞–Ω–∏–µ
            description = entry.get('summary', '') or entry.get('description', '')
            if description:
                import re
                description = re.sub(r'<[^>]+>', '', description)
                description = description.strip()
            
            # –ö–æ–Ω—Ç–µ–Ω—Ç
            content = ''
            if hasattr(entry, 'content') and entry.content:
                content = entry.content[0].value if entry.content else ''
                if content:
                    import re
                    content = re.sub(r'<[^>]+>', '', content).strip()
            
            # –ê–≤—Ç–æ—Ä
            author = entry.get('author', '')
            
            # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            published_date = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                utc_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                published_date = utc_date
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                utc_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                published_date = utc_date
            else:
                published_date = datetime.now(timezone.utc)
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ RSS
            categories = []
            if hasattr(entry, 'tags'):
                categories.extend([tag.term for tag in entry.tags if hasattr(tag, 'term')])
            if hasattr(entry, 'category'):
                categories.append(entry.category)
            
            return {
                'title': title,
                'link': link,
                'description': description,
                'content': content,
                'author': author,
                'published_date': published_date,
                'categories': categories
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å–∏: {e}")
            return None
    
    def _check_keywords(self, article_data, keywords=None):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—å–∏ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        if keywords is None:
            keywords = self.keywords
        
        if not keywords:
            return []
        
        matched_keywords = []
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
        search_texts = [
            article_data.get('title', '').lower(),
            article_data.get('description', '').lower(),
            article_data.get('content', '').lower(),
            article_data.get('author', '').lower(),
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ RSS
        categories_text = ' '.join(article_data.get('categories', [])).lower()
        search_texts.append(categories_text)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
        full_text = ' '.join(search_texts)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
        for keyword in keywords:
            keyword_lower = keyword.lower().strip()
            if keyword_lower and keyword_lower in full_text:
                matched_keywords.append(keyword)
        
        return matched_keywords
    
    async def _send_articles_to_telegram(self, articles, source_url):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å—Ç–∞—Ç–µ–π –≤ Telegram"""
        source_name = self._extract_domain_name(source_url)
        
        for article in articles:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º topic_id –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                topic_id = article.get('topic_id')
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–µ—Ä–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ TelegramSender
                success = self.telegram.send_article(
                    title=article['title'],
                    description=article.get('description', ''),
                    link=article.get('link', ''),
                    source=source_name,
                    keywords=article.get('matched_keywords', []),
                    categories=article.get('categories', []),
                    topic_id=topic_id
                )
                
                if success:
                    keywords_str = ', '.join(article.get('matched_keywords', [])[:2])
                    topic_str = f" ‚Üí —Ç–æ–ø–∏–∫ {topic_id}" if topic_id else ""
                    print(f"üì± –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {article['title'][:40]}... [{keywords_str}]{topic_str}")
                else:
                    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å: {article['title'][:40]}...")
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {e}")
    
    def _extract_domain_name(self, url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω–Ω–æ–µ –∏–º—è –∏–∑ URL –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        try:
            if "tass.ru" in url:
                return "–¢–ê–°–°"
            elif "lenta.ru" in url:
                return "Lenta.ru"
            elif "ria.ru" in url:
                return "–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏"
            elif "interfax.ru" in url:
                return "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å"
            elif "kommersant.ru" in url:
                return "–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç"
            elif "rbc.ru" in url:
                return "–†–ë–ö"
            else:
                from urllib.parse import urlparse
                domain = urlparse(url).netloc
                return domain.replace('www.', '')
        except:
            return "RSS –∏—Å—Ç–æ—á–Ω–∏–∫"
    
    def _record_feed_error(self, feed_url):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –æ—à–∏–±–∫—É –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        self.error_counts[feed_url] = self.error_counts.get(feed_url, 0) + 1
        self.last_error_time[feed_url] = time.time()
    
    def _reset_feed_errors(self, feed_url):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        if feed_url in self.error_counts:
            del self.error_counts[feed_url]
        if feed_url in self.last_error_time:
            del self.last_error_time[feed_url]
    
    def get_error_count(self, feed_url):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        return self.error_counts.get(feed_url, 0)
    
    def should_skip_feed(self, feed_url, max_errors=5):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å—Ç–æ–∏—Ç –ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫"""
        error_count = self.get_error_count(feed_url)
        if error_count >= max_errors:
            last_error = self.last_error_time.get(feed_url, 0)
            delay_minutes = min(60, 2 ** error_count)
            
            if time.time() - last_error < delay_minutes * 60:
                print(f"‚è∏Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é {feed_url} –Ω–∞ {delay_minutes} –º–∏–Ω—É—Ç (–æ—à–∏–±–æ–∫: {error_count})")
                return True
        
        return False